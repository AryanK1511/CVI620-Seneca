{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "42ffb510-730c-4141-ae2f-eca44e32ff28",
      "metadata": {
        "id": "42ffb510-730c-4141-ae2f-eca44e32ff28"
      },
      "source": [
        "# CVI620 Midterm Exam - Jupyter Notebook with all tasks and explanation\n",
        "\n",
        "- **Name:** Aryan Khurana\n",
        "- **Student ID:** 145282216"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5cff8af1-e189-4881-b5c1-188f6e616d80",
      "metadata": {
        "id": "5cff8af1-e189-4881-b5c1-188f6e616d80"
      },
      "source": [
        "## Step 0: Importing Libraries\n",
        "\n",
        "Here, I am importing libraries that will be used later in the program. I am also initializing a dictionary that maps the name of the images to the path."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "e9aa494a-1b40-4a3c-b8fb-c33124f9224b",
      "metadata": {
        "id": "e9aa494a-1b40-4a3c-b8fb-c33124f9224b"
      },
      "outputs": [],
      "source": [
        "import cv2 as cv\n",
        "import numpy as np\n",
        "\n",
        "image_hashmap = {\n",
        "    \"Geometric Shapes - OpenCV\": \"images/geometric_shapes.jpg\",\n",
        "    \"Industrial Objects - OpenCV\": \"images/industrial_objects.jpg\",\n",
        "    \"Natural Scenes - OpenCV\": \"images/natural_scenes.jpg\",\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "92d52ef3-5bb2-4233-a67c-c597d9d08e62",
      "metadata": {
        "id": "92d52ef3-5bb2-4233-a67c-c597d9d08e62"
      },
      "source": [
        "## Step 1: Acquiring Sample Images\n",
        "\n",
        "**Objective:** Obtain a set of images for testing edge detection methods. Images can be sourced from natural scenes, industrial objects, or geometric\n",
        "shapes to highlight various edge properties.\n",
        "\n",
        "### Explanation of my solution:\n",
        "\n",
        "Here, I am looping through the image mapping we created earlier and displaying all the images using `cv.imshow()`. I am also appending the images (numpy arrays) to the `images` list so I can use them later in the program.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4d00587a-ab46-4196-9e8d-433523e7b3d1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4d00587a-ab46-4196-9e8d-433523e7b3d1",
        "outputId": "1912ec47-5acc-4ca0-d5ac-630e35dec20a"
      },
      "outputs": [],
      "source": [
        "images = []\n",
        "\n",
        "for window_name, image_path in image_hashmap.items():\n",
        "    image = cv.imread(image_path)\n",
        "    if image is None:\n",
        "        print(f\"Error: Unable to load image from {image_path}\")\n",
        "        continue\n",
        "    images.append(image)\n",
        "    cv.imshow(window_name, image)\n",
        "\n",
        "cv.waitKey(0)\n",
        "cv.destroyAllWindows()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5fe20f39",
      "metadata": {
        "id": "5fe20f39"
      },
      "source": [
        "## Task 2: Grayscale Conversion\n",
        "\n",
        "**Objective:** Convert the color images into grayscale to simplify edge detection by focusing on intensity changes.\n",
        "\n",
        "### Explanation of my solution:\n",
        "\n",
        "Here, I am creating a list called `gray_images` and then looping through each image in the `images` array that I created earlier. I convert each image to grayscale one by one, append them to the `gray_images` list, and display the grayscale images.\n",
        "\n",
        "##### Why is grayscale is preferred for edge detection\n",
        "\n",
        "- Grayscale is preferred for edge detection because it simplifies the image by reducing it to a single channel of intensity values, making the process more efficient.\n",
        "- Edge detection focuses on identifying sharp changes in intensity between adjacent pixels, and since grayscale images represent only intensity, it highlights these changes without interference from color variations.\n",
        "- Processing a single grayscale channel also requires less computational power and helps reduce noise by removing unnecessary color information, leading to more accurate and consistent results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "00178cbd",
      "metadata": {
        "id": "00178cbd"
      },
      "outputs": [],
      "source": [
        "gray_images = []\n",
        "\n",
        "for i, image in enumerate(images):\n",
        "    gray_image = cv.cvtColor(image, cv.COLOR_BGR2GRAY)\n",
        "    gray_images.append(gray_image)\n",
        "    cv.imshow(f\"Grayscale - Image {i + 1}\", gray_image)\n",
        "\n",
        "cv.waitKey(0)\n",
        "cv.destroyAllWindows()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8abfca1d",
      "metadata": {
        "id": "8abfca1d"
      },
      "source": [
        "## Task 3: Noise Reduction Using Smoothing\n",
        "\n",
        "**Objective:** Apply noise reduction techniques to smooth the image and improve edge detection accuracy.\n",
        "\n",
        "### Explanation of my solution:\n",
        "\n",
        "Here, I am creating a list called `blurred_images` and then looping through each image in the `gray_images` array. For each grayscale image, I apply a Gaussian blur using `cv.GaussianBlur()` and append the blurred image to the `blurred_images` list.\n",
        "\n",
        "Additionally, I convert the grayscale images and their blurred counterparts to 3-channel images using `cv.cvtColor()` so that they can be concatenated with the original colored images. I then use `cv.hconcat()` to horizontally concatenate the original image, the grayscale image, and the blurred image into a single output. Finally, I display this concatenated image using `cv.imshow()`, showing the comparison between the original, grayscale, and Gaussian blurred versions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "6fc6b9ba",
      "metadata": {
        "id": "6fc6b9ba"
      },
      "outputs": [],
      "source": [
        "blurred_images = []\n",
        "for i in range(len(gray_images)):\n",
        "    gaussian_blur = cv.GaussianBlur(gray_images[i], (5, 5), 0)\n",
        "    blurred_images.append(gaussian_blur)\n",
        "\n",
        "    gray_image_3channel = cv.cvtColor(gray_images[i], cv.COLOR_GRAY2BGR)\n",
        "    gaussian_blur_3channel = cv.cvtColor(gaussian_blur, cv.COLOR_GRAY2BGR)\n",
        "\n",
        "    concatenated_image = cv.hconcat(\n",
        "        [images[i], gray_image_3channel, gaussian_blur_3channel]\n",
        "    )\n",
        "    cv.imshow(\n",
        "        f\"Original vs Grayscale vs Gaussian Blurred - Image {i + 1}\", concatenated_image\n",
        "    )\n",
        "\n",
        "cv.waitKey(0)\n",
        "cv.destroyAllWindows()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fb46ca5d",
      "metadata": {
        "id": "fb46ca5d"
      },
      "source": [
        "## Task 4: Sobel Edge Detection\n",
        "\n",
        "**Objective:** Detect edges using the Sobel operator, which calculates the gradient in both the x and y directions.\n",
        "\n",
        "### Explanation of My Solution:\n",
        "\n",
        "In this segment, I loop through each image in the `blurred_images` list and apply Sobel edge detection to identify both horizontal and vertical edges. I experiment with three different kernel sizes: `3`, `5`, and `7`.\n",
        "\n",
        "For each kernel size, I compute the gradient of the image along the x-axis using `cv.Sobel()` with parameters `1, 0` and along the y-axis with parameters `0, 1`. This generates two separate gradient images: `sobel_x` for horizontal edges and `sobel_y` for vertical edges.\n",
        "\n",
        "Next, I combine these gradients by taking the weighted sum of the absolute values of both `sobel_x` and `sobel_y` using `cv.addWeighted()`, resulting in a combined image, `sobel_combined`, that shows the overall edges in the image. Finally, I convert the combined result to an 8-bit format using `np.uint8()` and display the Sobel edge-detected image with `cv.imshow()`.\n",
        "\n",
        "### Sobel Edge Detection and Kernel Sizes\n",
        "\n",
        "Sobel edge detection identifies edges in a picture by leveraging the pixel intensity along both the horizontal (x-axis) and vertical (y-axis) directions.\n",
        "Larger kernels provide a smoother, more generalized view of edges but may miss finer details, while smaller kernels can capture intricate edges at the risk of introducing noise.\n",
        "\n",
        "**Experimenting with Kernel Sizes**:\n",
        "\n",
        "   - The kernel size in the Sobel operator determines how much surrounding pixel information is considered when calculating gradients. A larger kernel size captures more context from the image, leading to smoother gradients but potentially blurring smaller features, while a smaller kernel focuses on fine details.\n",
        "   \n",
        "   - **Larger Kernel Sizes**:\n",
        "     - A larger kernel size (e.g., 7x7 or 9x9) tends to smooth out noise even more and can help identify broader edges more clearly.\n",
        "     - However, this might cause smaller edges or details to be overlooked or blended into larger features. Consequently, smaller objects or intricate edges may not be detected as clearly, resulting in a loss of detail in the edge map.\n",
        "   \n",
        "   - **Smaller Kernel Sizes**:\n",
        "     - A smaller kernel size (e.g., 3x3 or 5x5) is better suited for detecting fine edges and intricate details within the image. It captures rapid changes in intensity more effectively, making it easier to identify small features.\n",
        "     - The downside is that smaller kernels are more sensitive to noise, which can introduce false edges or artifacts in the resulting edge map. This might lead to an edge map that appears cluttered or noisy.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "adcf0d53",
      "metadata": {
        "id": "adcf0d53"
      },
      "outputs": [],
      "source": [
        "kernel_sizes = [3, 5, 7]\n",
        "sobel_images = []\n",
        "\n",
        "for index, blurred_image in enumerate(blurred_images):\n",
        "    for ksize in kernel_sizes:\n",
        "        sobel_x = cv.Sobel(blurred_image, cv.CV_64F, 1, 0, ksize=ksize)\n",
        "        sobel_y = cv.Sobel(blurred_image, cv.CV_64F, 0, 1, ksize=ksize)\n",
        "\n",
        "        sobel_combined = cv.addWeighted(np.abs(sobel_x), 0.5, np.abs(sobel_y), 0.5, 0)\n",
        "\n",
        "        sobel_combined = np.uint8(sobel_combined)\n",
        "        \n",
        "        if ksize == 3:\n",
        "            sobel_images.append(sobel_combined)\n",
        "            \n",
        "        window_name = f\"Sobel Edge Detection - Kernel Size: {ksize}\"\n",
        "        cv.imshow(window_name, sobel_combined)\n",
        "\n",
        "cv.waitKey(0)\n",
        "cv.destroyAllWindows()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7c42cead",
      "metadata": {},
      "source": [
        "## Task 5: Laplacian Edge Detection\n",
        "\n",
        "**Objective:** Detect edges using the Laplacian operator, which highlights regions of rapid intensity change in an image.\n",
        "\n",
        "### Explanation of My Solution:\n",
        "\n",
        "In this segment, I loop through each image in the `blurred_images` list and apply the Laplacian edge detection technique to identify edges based on intensity gradients. The Laplacian operator computes the second derivative of the image intensity, effectively measuring the rate of change of the gradient.\n",
        "\n",
        "The Laplacian operator is particularly effective at highlighting edges because it responds to regions where there is a rapid change in intensity, regardless of the direction of the change. Unlike the Sobel operator, which calculates gradients in specific directions (horizontal and vertical), the Laplacian operator provides a more generalized approach by considering the second derivative. \n",
        "\n",
        "#### Laplacian Second Derivative v/s Sobel Gradient Calculation\n",
        "\n",
        "- The Laplacian operator calculates the second derivative of image intensity, making it sensitive to noise and effective at detecting edges regardless of direction, which can result in a cluttered output with false edges. \n",
        "- In contrast, the Sobel operator computes the first derivative in both horizontal and vertical directions, producing clearer and more defined edges with reduced noise sensitivity due to its averaging of pixel intensities. \n",
        "- While the Laplacian can highlight a broader range of intensity changes, Sobel is often preferred in practical applications for its robustness and directional sensitivity, making it particularly effective in scenarios requiring precise feature extraction and object detection."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "33552e78",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "33552e78",
        "outputId": "1eff9416-458d-4e39-8c6e-164f4b8ce231"
      },
      "outputs": [],
      "source": [
        "laplacian_images = []\n",
        "\n",
        "for index, blurred_image in enumerate(blurred_images):\n",
        "    laplacian = cv.Laplacian(blurred_image, cv.CV_64F)\n",
        "    laplacian = np.abs(laplacian)\n",
        "    laplacian = np.uint8(laplacian)\n",
        "    laplacian_images.append(laplacian)\n",
        "    cv.imshow(f\"Laplacian Edge Detection - Image {index + 1}\", laplacian)\n",
        "\n",
        "cv.waitKey(0)\n",
        "cv.destroyAllWindows()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b65d7f9e",
      "metadata": {},
      "source": [
        "In this code snippet, I iterate through pairs of Sobel and Laplacian edge-detected images, converting each to a three-channel format for display. I print their shapes and data types, then ensure the Laplacian image matches the Sobel image's dimensions by resizing if necessary. Finally, I concatenate the two images horizontally and display them in a window labeled \"Sobel vs Laplacian - Image X\" for visual comparison of their edge detection results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "f6462345",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f6462345",
        "outputId": "630abc14-19d1-439c-ccd3-533fe505ae1e"
      },
      "outputs": [],
      "source": [
        "for i in range(len(sobel_images)):\n",
        "    sobel_image_3channel = cv.cvtColor(sobel_images[i], cv.COLOR_GRAY2BGR)\n",
        "    laplacian_image_3channel = cv.cvtColor(laplacian_images[i], cv.COLOR_GRAY2BGR)\n",
        "\n",
        "    if laplacian_image_3channel.shape[:2] != sobel_image_3channel.shape[:2]:\n",
        "        laplacian_image_3channel = cv.resize(laplacian_image_3channel, (sobel_image_3channel.shape[1], sobel_image_3channel.shape[0]))\n",
        "\n",
        "    concatenated_image = cv.hconcat([sobel_image_3channel, laplacian_image_3channel])\n",
        "    cv.imshow(f\"Sobel vs Laplacian - Image {i + 1}\", concatenated_image)\n",
        "\n",
        "\n",
        "cv.waitKey(0)\n",
        "cv.destroyAllWindows()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4c3f99fa",
      "metadata": {},
      "source": [
        "## Task 7: Canny Edge Detection\n",
        "\n",
        "**Objective:** Implement the Canny edge detection algorithm, which is a multi-stage process for detecting strong and weak edges.\n",
        "\n",
        "Here's an explanation of your Canny edge detection code:\n",
        "\n",
        "### Explanation of My Solution\n",
        "\n",
        "In this segment, I loop through each image in the `blurred_images` list and apply the Canny edge detection technique with different threshold values, stored in the `thresholds` list. The thresholds determine the sensitivity of the edge detector, with lower values detecting more edges (including noise) and higher values focusing only on stronger edges.\n",
        "\n",
        "I define three different pairs of thresholds: `(50, 150)`, `(100, 200)`, and `(150, 250)`, to explore the impact of these values on the edge detection results.\n",
        "\n",
        "For each image, I apply the `cv.Canny()` function using each threshold pair and display the result using `cv.imshow()`. However, I limit the `canny_images` list to store only the first 3 images produced. This allows me to experiment with all the threshold values for every image, but only keep a small subset of the results for further analysis and comparison.\n",
        "\n",
        "Finally, I display all the Canny-detected edges for each image, and once the comparisons are done, I clean up using `cv.waitKey(0)` and `cv.destroyAllWindows()`.\n",
        "\n",
        "### Canny Edge Detection and Threshold Values\n",
        "\n",
        "Canny edge detection identifies edges by calculating the intensity gradient of an image and tracing the edges where intensity changes are the most prominent. The edge detection is controlled by two threshold values: a lower threshold that identifies weak edges and a higher threshold for strong edges. \n",
        "\n",
        "**Experimenting with Threshold Values**:\n",
        "\n",
        "   - The threshold values in the Canny operator determine the sensitivity of edge detection. Lower thresholds detect more edges, including weaker ones, which may include noise. Higher thresholds, on the other hand, detect only stronger, more prominent edges, potentially missing finer details.\n",
        "\n",
        "   - **Lower Thresholds**:\n",
        "     - A lower threshold (e.g., 50) makes the algorithm more sensitive to edges, detecting even faint or weak edges. This can be helpful in images where fine details are important, but it may also result in more noise or false edges being detected.\n",
        "     - The risk of using lower thresholds is the introduction of noise, as it can mistake small changes in intensity for meaningful edges.\n",
        "   \n",
        "   - **Higher Thresholds**:\n",
        "     - A higher threshold (e.g., 150) makes the edge detector focus on more prominent edges, reducing noise and clutter. This results in a cleaner edge map but can potentially overlook weaker edges or finer details.\n",
        "     - The downside of higher thresholds is that smaller or less distinct edges might be missed, leading to a more simplified and less detailed edge map.\n",
        "    \n",
        "### Canny v/s Sobel v/s Laplacian\n",
        "\n",
        "- **Canny edge detection** is considered the most accurate because it uses a series of steps to effectively identify edges. It first smooths the image to reduce noise, which helps prevent false edges from appearing. Then, it finds areas of rapid intensity change and refines these edges to make them sharp and well-defined. However, this process requires more computation, making it slower.\n",
        "- **Sobel edge detection**, on the other hand, is faster and simpler. It focuses on calculating how quickly pixel values change in both horizontal and vertical directions. While this method can quickly identify edges, it may not be as precise because it can blend thinner edges together or get confused by noise in the image.\n",
        "- **Laplacian edge detection** highlights regions of rapid intensity change but is more sensitive to noise, which can lead to more false edges. Itâ€™s quicker than Canny but may produce less accurate results.\n",
        "\n",
        "> **Overall, Canny is great for accuracy but takes longer, while Sobel and Laplacian are faster but might not catch all the details as clearly.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "03cc84e5-0c14-45b4-8607-01848e4a6aa9",
      "metadata": {
        "id": "03cc84e5-0c14-45b4-8607-01848e4a6aa9"
      },
      "outputs": [],
      "source": [
        "canny_images = []\n",
        "\n",
        "thresholds = [(50, 150), (100, 200), (150, 250)] \n",
        "\n",
        "for index, blurred_image in enumerate(blurred_images):\n",
        "    for low_thresh, high_thresh in thresholds:\n",
        "        canny_edges = cv.Canny(blurred_image, low_thresh, high_thresh)\n",
        "\n",
        "        if low_thresh == 50:\n",
        "            canny_images.append(canny_edges)\n",
        "\n",
        "        cv.imshow(f\"Canny Edge Detection - Image {index + 1} (Thresh: {low_thresh}, {high_thresh})\", canny_edges)\n",
        "\n",
        "cv.waitKey(0)\n",
        "cv.destroyAllWindows()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "50dfb968",
      "metadata": {},
      "source": [
        "In this code, I loop through the `sobel_images`, `laplacian_images`, and `canny_images` lists, ensuring that each image from the Laplacian and Canny methods is resized to match the dimensions of the corresponding Sobel image. Then, I concatenate the Sobel, Laplacian, and Canny edge-detected images horizontally for each index, displaying the combined result in a single window for side-by-side comparison of the different edge detection methods."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a8b4db26-d888-4836-941f-dce32c1e0dc8",
      "metadata": {
        "id": "a8b4db26-d888-4836-941f-dce32c1e0dc8"
      },
      "outputs": [],
      "source": [
        "for i in range(len(sobel_images)):\n",
        "    sobel_image = sobel_images[i]\n",
        "    laplacian_image = laplacian_images[i]\n",
        "    canny_image = canny_images[i]\n",
        "    height, width = sobel_image.shape[:2]\n",
        "\n",
        "    if laplacian_image.shape[:2] != (height, width):\n",
        "        laplacian_image = cv.resize(laplacian_image, (width, height))\n",
        "\n",
        "    if canny_image.shape[:2] != (height, width):\n",
        "        canny_image = cv.resize(canny_image, (width, height))\n",
        "\n",
        "    concatenated_image = cv.hconcat([sobel_image, laplacian_image, canny_image])\n",
        "    \n",
        "    cv.imshow(f\"Sobel vs Laplacian vs Canny - Image {i + 1}\", concatenated_image)\n",
        "\n",
        "cv.waitKey(0)\n",
        "cv.destroyAllWindows()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "socv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
